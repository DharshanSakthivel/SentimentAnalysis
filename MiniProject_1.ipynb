{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ef7ad19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\12601\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\12601\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\12601\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "c:\\Users\\12601\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\12601\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ====== [1] Imports and Setup ======\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast, DistilBertForSequenceClassification,\n",
    "    BertTokenizerFast, BertForSequenceClassification,\n",
    "    RobertaTokenizerFast, RobertaForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9834748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== [2] Directory Setup ======\n",
    "BASE_DIR = os.path.join(\".\", \"saved_models\")\n",
    "TRADITIONAL_DIR = os.path.join(BASE_DIR, \"traditional\")\n",
    "TRANSFORMERS_DIR = os.path.join(BASE_DIR, \"transformers\")\n",
    "os.makedirs(TRADITIONAL_DIR, exist_ok=True)\n",
    "os.makedirs(TRANSFORMERS_DIR, exist_ok=True)\n",
    "\n",
    "# Clear Hugging Face cache to avoid corrupted/incomplete models\n",
    "hf_cache_dir = os.path.expanduser(\"~/.cache/huggingface/transformers\")\n",
    "if os.path.exists(hf_cache_dir):\n",
    "    print(\"\\nðŸ§¹ Clearing Hugging Face Transformers cache...\")\n",
    "    shutil.rmtree(hf_cache_dir)\n",
    "    print(\"âœ… Cache cleared successfully.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c62a2fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== [3] Emoticon Dictionary ======\n",
    "emoticon_dict = {\n",
    "    \":)\": \"smiley\", \":-)\": \"smiley\", \":D\": \"laugh\", \":-D\": \"laugh\",\n",
    "    \":(\": \"sad\", \":-(\": \"sad\", \":'(\": \"cry\", \":'-(\": \"cry\",\n",
    "    \":P\": \"playful\", \":-P\": \"playful\", \";)\": \"wink\", \";-)\": \"wink\",\n",
    "    \":/\": \"skeptical\", \":-/\": \"skeptical\", \":|\": \"neutral\", \":-|\": \"neutral\",\n",
    "    \":O\": \"surprised\", \":-O\": \"surprised\", \"XD\": \"laugh\", \"<3\": \"love\",\n",
    "    \">:(\": \"angry\", \"D:\": \"horrified\", \":-*\": \"kiss\", \":3\": \"cute\",\n",
    "    \":-X\": \"sealed lips\", \"B-)\": \"cool\", \"O:)\": \"angel\", \">:)\": \"evil smile\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3741e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== [4] Load Dataset ======\n",
    "train_df = pd.read_csv(\"C:/Users/12601/OneDrive/Desktop/collage works/mini prj/train.csv\")\n",
    "test_df = pd.read_csv(\"C:/Users/12601/OneDrive/Desktop/collage works/mini prj/test.csv\")\n",
    "\n",
    "train_df = train_df[train_df[\"sentiment\"] != \"neutral\"]\n",
    "test_df = test_df[test_df[\"sentiment\"] != \"neutral\"]\n",
    "\n",
    "label_map = {\"negative\": 0, \"positive\": 1}\n",
    "train_df[\"label\"] = train_df[\"sentiment\"].map(label_map)\n",
    "test_df[\"label\"] = test_df[\"sentiment\"].map(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f0aaa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== [5] Preprocessing Function ======\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\\\S+\", \"link\", text)\n",
    "    text = re.sub(r\"@\\\\w+\", \"user_mention\", text)\n",
    "    text = re.sub(r\"#(\\\\w+)\", lambda m: m.group(1).replace(\"_\", \" \"), text)\n",
    "    text = re.sub(r\"\\\\d+\", \"\", text)\n",
    "\n",
    "    for emot, meaning in emoticon_dict.items():\n",
    "        text = text.replace(emot, f\" {meaning} \")\n",
    "\n",
    "    text = emoji.demojize(text)\n",
    "    text = text.replace(\":\", \"\").replace(\"_\", \" \")\n",
    "    text = re.sub(rf\"[{string.punctuation}]\", \"\", text)\n",
    "    text = re.sub(r\"(.)\\\\1{2,}\", r\"\\\\1\\\\1\", text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.difference_update({\"no\", \"not\", \"don\", \"shouldn\", \"wasn\", \"mustn\"})\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(lemmatizer.lemmatize(w)) for w in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "train_df[\"clean_text\"] = train_df[\"text\"].apply(preprocess)\n",
    "test_df[\"clean_text\"] = test_df[\"text\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8313c35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NB with count_bi ===\n",
      "Accuracy: 0.8944866920152091\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89      1001\n",
      "           1       0.89      0.91      0.90      1103\n",
      "\n",
      "    accuracy                           0.89      2104\n",
      "   macro avg       0.89      0.89      0.89      2104\n",
      "weighted avg       0.89      0.89      0.89      2104\n",
      "\n",
      "\n",
      "=== LR with count_bi ===\n",
      "Accuracy: 0.9011406844106464\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90      1001\n",
      "           1       0.92      0.88      0.90      1103\n",
      "\n",
      "    accuracy                           0.90      2104\n",
      "   macro avg       0.90      0.90      0.90      2104\n",
      "weighted avg       0.90      0.90      0.90      2104\n",
      "\n",
      "\n",
      "=== SVM with count_bi ===\n",
      "Accuracy: 0.8968631178707225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89      1001\n",
      "           1       0.91      0.89      0.90      1103\n",
      "\n",
      "    accuracy                           0.90      2104\n",
      "   macro avg       0.90      0.90      0.90      2104\n",
      "weighted avg       0.90      0.90      0.90      2104\n",
      "\n",
      "\n",
      "=== NB with tfidf_bi ===\n",
      "Accuracy: 0.8916349809885932\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.86      0.88      1001\n",
      "           1       0.88      0.92      0.90      1103\n",
      "\n",
      "    accuracy                           0.89      2104\n",
      "   macro avg       0.89      0.89      0.89      2104\n",
      "weighted avg       0.89      0.89      0.89      2104\n",
      "\n",
      "\n",
      "=== LR with tfidf_bi ===\n",
      "Accuracy: 0.8954372623574145\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.90      0.89      1001\n",
      "           1       0.91      0.89      0.90      1103\n",
      "\n",
      "    accuracy                           0.90      2104\n",
      "   macro avg       0.90      0.90      0.90      2104\n",
      "weighted avg       0.90      0.90      0.90      2104\n",
      "\n",
      "\n",
      "=== SVM with tfidf_bi ===\n",
      "Accuracy: 0.9035171102661597\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90      1001\n",
      "           1       0.91      0.90      0.91      1103\n",
      "\n",
      "    accuracy                           0.90      2104\n",
      "   macro avg       0.90      0.90      0.90      2104\n",
      "weighted avg       0.90      0.90      0.90      2104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ====== [6] Feature Extraction and Traditional Model Training ======\n",
    "vectorizers = {\n",
    "    \"count_bi\": CountVectorizer(ngram_range=(1, 2)),\n",
    "    \"tfidf_bi\": TfidfVectorizer(ngram_range=(1, 2))\n",
    "}\n",
    "\n",
    "X_train_texts = train_df[\"clean_text\"]\n",
    "y_train = train_df[\"label\"]\n",
    "X_test_texts = test_df[\"clean_text\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "for name, vectorizer in vectorizers.items():\n",
    "    X_train = vectorizer.fit_transform(X_train_texts)\n",
    "    X_test = vectorizer.transform(X_test_texts)\n",
    "\n",
    "    for model_name, model in {\n",
    "        \"nb\": MultinomialNB(),\n",
    "        \"lr\": LogisticRegression(max_iter=1000),\n",
    "        \"svm\": SVC(kernel=\"linear\", probability=True)\n",
    "    }.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        print(f\"\\n=== {model_name.upper()} with {name} ===\")\n",
    "        print(\"Accuracy:\", accuracy_score(y_test, preds))\n",
    "        print(classification_report(y_test, preds))\n",
    "        joblib.dump((model, vectorizer), os.path.join(TRADITIONAL_DIR, f\"{model_name}_{name}.joblib\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d17b3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== [7] Universal Transformer Trainer ======\n",
    "def train_transformer(model_cls, tokenizer_cls, model_name, train_texts, test_texts, y_train, y_test):\n",
    "    print(f\"\\nðŸš€ Training {model_name}...\")\n",
    "    tokenizer = tokenizer_cls.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize(batch):\n",
    "        return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "    train_ds = Dataset.from_pandas(pd.DataFrame({\"text\": train_texts, \"label\": y_train}))\n",
    "    test_ds = Dataset.from_pandas(pd.DataFrame({\"text\": test_texts, \"label\": y_test}))\n",
    "    train_ds = train_ds.map(tokenize, batched=True).remove_columns([\"text\"])\n",
    "    test_ds = test_ds.map(tokenize, batched=True).remove_columns([\"text\"])\n",
    "\n",
    "    model = model_cls.from_pretrained(model_name, num_labels=2)\n",
    "    output_path = os.path.join(TRANSFORMERS_DIR, model_name.replace(\"/\", \"_\"))\n",
    "\n",
    "    is_large = \"large\" in model_name\n",
    "    if is_large:\n",
    "        args = TrainingArguments(\n",
    "            output_dir=output_path,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir=os.path.join(output_path, \"logs\"),\n",
    "            per_device_train_batch_size=4,\n",
    "            per_device_eval_batch_size=4,\n",
    "            gradient_accumulation_steps=4,\n",
    "            num_train_epochs=2,\n",
    "            learning_rate=2e-5,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=100,\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=1,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            fp16=torch.cuda.is_available()\n",
    "        )\n",
    "    else:\n",
    "        args = TrainingArguments(\n",
    "            output_dir=output_path,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            logging_dir=os.path.join(output_path, \"logs\"),\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            num_train_epochs=2,\n",
    "            load_best_model_at_end=True,\n",
    "            save_total_limit=1,\n",
    "            metric_for_best_model=\"accuracy\",\n",
    "            fp16=torch.cuda.is_available()\n",
    "        )\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(output_path)\n",
    "    tokenizer.save_pretrained(output_path)\n",
    "    print(f\"\\nâœ… {model_name} Evaluation Complete\")\n",
    "    print(trainer.evaluate())\n",
    "\n",
    "    preds = trainer.predict(test_ds)\n",
    "    pred_labels = np.argmax(preds.predictions, axis=1)\n",
    "    print(classification_report(y_test, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34441e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training distilbert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16363/16363 [00:01<00:00, 12204.13 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2104/2104 [00:00<00:00, 14314.66 examples/s]\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\12601\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4092' max='4092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4092/4092 12:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.247100</td>\n",
       "      <td>0.214560</td>\n",
       "      <td>0.934886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.115300</td>\n",
       "      <td>0.245335</td>\n",
       "      <td>0.942015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… distilbert-base-uncased Evaluation Complete\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24533484876155853, 'eval_accuracy': 0.9420152091254753, 'eval_runtime': 23.5182, 'eval_samples_per_second': 89.463, 'eval_steps_per_second': 11.183, 'epoch': 2.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      1001\n",
      "           1       0.95      0.94      0.94      1103\n",
      "\n",
      "    accuracy                           0.94      2104\n",
      "   macro avg       0.94      0.94      0.94      2104\n",
      "weighted avg       0.94      0.94      0.94      2104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ====== [8] Run DistilBERT ======\n",
    "train_transformer(\n",
    "    DistilBertForSequenceClassification,\n",
    "    DistilBertTokenizerFast,\n",
    "    \"distilbert-base-uncased\",\n",
    "    train_df[\"text\"],\n",
    "    test_df[\"text\"],\n",
    "    y_train,\n",
    "    y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09cdf1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training bert-base-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16363/16363 [00:01<00:00, 11739.64 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2104/2104 [00:00<00:00, 11089.07 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\12601\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4092' max='4092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4092/4092 22:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.232237</td>\n",
       "      <td>0.935837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121400</td>\n",
       "      <td>0.248122</td>\n",
       "      <td>0.944392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… bert-base-uncased Evaluation Complete\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24812234938144684, 'eval_accuracy': 0.9443916349809885, 'eval_runtime': 38.9599, 'eval_samples_per_second': 54.004, 'eval_steps_per_second': 6.751, 'epoch': 2.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94      1001\n",
      "           1       0.96      0.94      0.95      1103\n",
      "\n",
      "    accuracy                           0.94      2104\n",
      "   macro avg       0.94      0.94      0.94      2104\n",
      "weighted avg       0.94      0.94      0.94      2104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ====== [9] Run BERT BASE ======\n",
    "train_transformer(\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizerFast,\n",
    "    \"bert-base-uncased\",\n",
    "    train_df[\"text\"],\n",
    "    test_df[\"text\"],\n",
    "    y_train,\n",
    "    y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3acaa242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training bert-large-uncased...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16363/16363 [00:01<00:00, 12546.19 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2104/2104 [00:00<00:00, 12632.22 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2044' max='2044' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2044/2044 4:42:18, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.198201</td>\n",
       "      <td>0.947719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… bert-large-uncased Evaluation Complete\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1982010304927826, 'eval_accuracy': 0.9477186311787072, 'eval_runtime': 333.0969, 'eval_samples_per_second': 6.316, 'eval_steps_per_second': 1.579, 'epoch': 1.9982889269127353}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.95      0.95      1001\n",
      "           1       0.96      0.94      0.95      1103\n",
      "\n",
      "    accuracy                           0.95      2104\n",
      "   macro avg       0.95      0.95      0.95      2104\n",
      "weighted avg       0.95      0.95      0.95      2104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ====== [10] Run BERT LARGE ======\n",
    "train_transformer(\n",
    "    BertForSequenceClassification,\n",
    "    BertTokenizerFast,\n",
    "    \"bert-large-uncased\",\n",
    "    train_df[\"text\"],\n",
    "    test_df[\"text\"],\n",
    "    y_train,\n",
    "    y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90b23094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training roberta-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16363/16363 [00:00<00:00, 16409.75 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2104/2104 [00:00<00:00, 14640.70 examples/s]\n",
      "c:\\Users\\12601\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\12601\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\12601\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4092' max='4092' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4092/4092 26:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.293000</td>\n",
       "      <td>0.340182</td>\n",
       "      <td>0.918726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.224493</td>\n",
       "      <td>0.944867</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… roberta-base Evaluation Complete\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2244928628206253, 'eval_accuracy': 0.9448669201520913, 'eval_runtime': 25.7646, 'eval_samples_per_second': 81.663, 'eval_steps_per_second': 10.208, 'epoch': 2.0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.94      1001\n",
      "           1       0.96      0.94      0.95      1103\n",
      "\n",
      "    accuracy                           0.94      2104\n",
      "   macro avg       0.94      0.95      0.94      2104\n",
      "weighted avg       0.95      0.94      0.94      2104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ====== [11] Run RoBERTa (Optional) ======\n",
    "# Uncomment when your internet connection is stable\n",
    "train_transformer(\n",
    "    RobertaForSequenceClassification,\n",
    "    RobertaTokenizerFast,\n",
    "   \"roberta-base\",\n",
    "    train_df[\"text\"],\n",
    "    test_df[\"text\"],\n",
    "    y_train,\n",
    "    y_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d96cfc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12601\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\12601\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\12601\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\12601\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: nb_count, lr_count, svm_count, distilbert-base-uncased, bert-base-uncased\n",
      "\n",
      "Text 1: tomorrow college is holiday\n",
      "â†’ Sentiment: negative\n",
      "\n",
      "Text 2: the class got cancelled and students got time to play\n",
      "â†’ Sentiment: negative\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# ===== Preprocessing Function =====\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words.difference_update({\"no\", \"not\", \"don\", \"shouldn\", \"wasn\", \"mustn\"})\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "emoticon_dict = {\n",
    "    \":)\": \"smiley\", \":-)\": \"smiley\", \":D\": \"laugh\", \":-D\": \"laugh\",\n",
    "    \":(\": \"sad\", \":-(\": \"sad\", \":'(\": \"cry\", \":'-(\": \"cry\",\n",
    "    \":P\": \"playful\", \":-P\": \"playful\", \";)\": \"wink\", \";-)\": \"wink\",\n",
    "    \":/\": \"skeptical\", \":-/\": \"skeptical\", \":|\": \"neutral\", \":-|\": \"neutral\",\n",
    "    \":O\": \"surprised\", \":-O\": \"surprised\", \"XD\": \"laugh\", \"<3\": \"love\",\n",
    "    \">:(\": \"angry\", \"D:\": \"horrified\", \":-*\": \"kiss\", \":3\": \"cute\",\n",
    "    \":-X\": \"sealed lips\", \"B-)\": \"cool\", \"O:)\": \"angel\", \">:)\": \"evil smile\"\n",
    "}\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\\\S+\", \"link\", text)\n",
    "    text = re.sub(r\"@\\\\w+\", \"user_mention\", text)\n",
    "    text = re.sub(r\"#(\\\\w+)\", lambda m: m.group(1).replace(\"_\", \" \"), text)\n",
    "    text = re.sub(r\"\\\\d+\", \"\", text)\n",
    "\n",
    "    for emot, meaning in emoticon_dict.items():\n",
    "        text = text.replace(emot, f\" {meaning} \")\n",
    "\n",
    "    text = emoji.demojize(text).replace(\":\", \"\").replace(\"_\", \" \")\n",
    "    text = re.sub(rf\"[{string.punctuation}]\", \"\", text)\n",
    "    text = re.sub(r\"(.)\\\\1{2,}\", r\"\\\\1\\\\1\", text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    tokens = [stemmer.stem(lemmatizer.lemmatize(w)) for w in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# ===== Load Model Function =====\n",
    "def predict_sentiment(texts, model_type=\"distilbert\"):\n",
    "    results = []\n",
    "    if model_type in [\"nb_count\", \"lr_count\", \"svm_count\"]:\n",
    "        model, vectorizer = joblib.load(f\"C:/Users/12601/OneDrive/Desktop/collage works/mini prj/code/saved_models/traditional/{model_type}_bi.joblib\")\n",
    "        for text in texts:\n",
    "            text_clean = preprocess(text)\n",
    "            vec = vectorizer.transform([text_clean])\n",
    "            pred = model.predict(vec)[0]\n",
    "            results.append(\"positive\" if pred == 1 else \"negative\")\n",
    "    else:\n",
    "        model_path = f\"D:/saved_models/transformers/{model_type}\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "        model.eval()\n",
    "        for text in texts:\n",
    "            with torch.no_grad():\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "                outputs = model(**inputs)\n",
    "                pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "                results.append(\"positive\" if pred == 1 else \"negative\")\n",
    "    return results\n",
    "\n",
    "# ===== Interactive Input =====\n",
    "available_models = [\"nb_count\", \"lr_count\", \"svm_count\", \"distilbert-base-uncased\", \"bert-base-uncased\"]  # add more if saved\n",
    "print(\"Available models:\", \", \".join(available_models))\n",
    "model_choice = input(\"Enter the model to use: \").strip()\n",
    "\n",
    "num_inputs = int(input(\"How many texts do you want to analyze? \"))\n",
    "texts = [input(f\"Text {i+1}: \") for i in range(num_inputs)]\n",
    "\n",
    "# Normalize Hugging Face model name if needed\n",
    "hf_model_map = {\n",
    "    \"distilbert-base-uncased\": \"distilbert-base-uncased\",\n",
    "    \"bert-base-uncased\": \"bert-base-uncased\"\n",
    "}\n",
    "model_key = hf_model_map.get(model_choice, model_choice)\n",
    "\n",
    "# Predict\n",
    "sentiments = predict_sentiment(texts, model_key)\n",
    "\n",
    "# Output\n",
    "for i, (text, sentiment) in enumerate(zip(texts, sentiments), 1):\n",
    "    print(f\"\\nText {i}: {text}\\nâ†’ Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "063ce2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\12601\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\12601\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\12601\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available Models:\n",
      "1. Naive Bayes (CountVec)\n",
      "2. Logistic Regression (CountVec)\n",
      "3. SVM (CountVec)\n",
      "4. DistilBERT\n",
      "5. BERT Base\n",
      "6. BERT Large\n",
      "7. RoBERTa Base\n",
      "\n",
      "=== Predictions ===\n",
      "1. i can't â†’ positive\n",
      "2. his grandmother passed away  â†’ negative\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# ===== Preprocessing =====\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words.difference_update({\"no\", \"not\", \"don\", \"shouldn\", \"wasn\", \"mustn\"})\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "emoticon_dict = {\n",
    "    \":)\": \"smiley\", \":-)\": \"smiley\", \":D\": \"laugh\", \":-D\": \"laugh\",\n",
    "    \":(\": \"sad\", \":-(\": \"sad\", \":'(\": \"cry\", \":'-(\": \"cry\",\n",
    "    \":P\": \"playful\", \":-P\": \"playful\", \";)\": \"wink\", \";-)\": \"wink\",\n",
    "    \":/\": \"skeptical\", \":-/\": \"skeptical\", \":|\": \"neutral\", \":-|\": \"neutral\",\n",
    "    \":O\": \"surprised\", \":-O\": \"surprised\", \"XD\": \"laugh\", \"<3\": \"love\",\n",
    "    \">:(\": \"angry\", \"D:\": \"horrified\", \":-*\": \"kiss\", \":3\": \"cute\",\n",
    "    \":-X\": \"sealed lips\", \"B-)\": \"cool\", \"O:)\": \"angel\", \">:)\": \"evil smile\"\n",
    "}\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \"link\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"user_mention\", text)\n",
    "    text = re.sub(r\"#(\\w+)\", lambda m: m.group(1).replace(\"_\", \" \"), text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    for emot, meaning in emoticon_dict.items():\n",
    "        text = text.replace(emot, f\" {meaning} \")\n",
    "\n",
    "    text = emoji.demojize(text).replace(\":\", \"\").replace(\"_\", \" \")\n",
    "    text = re.sub(rf\"[{string.punctuation}]\", \"\", text)\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    tokens = [stemmer.stem(lemmatizer.lemmatize(w)) for w in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# ===== Prediction =====\n",
    "def predict_sentiment(texts, model_type):\n",
    "    results = []\n",
    "\n",
    "    # Traditional Models\n",
    "    if model_type in [\"nb_count\", \"lr_count\", \"svm_count\"]:\n",
    "        model, vectorizer = joblib.load(f\"C:/Users/12601/OneDrive/Desktop/collage works/mini prj/code/saved_models/traditional/{model_type}_bi.joblib\")\n",
    "        for text in texts:\n",
    "            text_clean = preprocess(text)\n",
    "            vec = vectorizer.transform([text_clean])\n",
    "            pred = model.predict(vec)[0]\n",
    "            results.append(\"positive\" if pred == 1 else \"negative\")\n",
    "\n",
    "    # Transformer Models\n",
    "    else:\n",
    "        model_path = f\"D:/saved_models/transformers/{model_type}\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path, local_files_only=True)\n",
    "        model.eval()\n",
    "        for text in texts:\n",
    "            with torch.no_grad():\n",
    "                inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "                outputs = model(**inputs)\n",
    "                pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "                results.append(\"positive\" if pred == 1 else \"negative\")\n",
    "\n",
    "    return results\n",
    "\n",
    "# ===== User Interaction =====\n",
    "all_models = {\n",
    "    \"Naive Bayes (CountVec)\": \"nb_count\",\n",
    "    \"Logistic Regression (CountVec)\": \"lr_count\",\n",
    "    \"SVM (CountVec)\": \"svm_count\",\n",
    "    \"DistilBERT\": \"distilbert-base-uncased\",\n",
    "    \"BERT Base\": \"bert-base-uncased\",\n",
    "    \"BERT Large\": \"bert-large-uncased\",\n",
    "    \"RoBERTa Base\": \"roberta-base\"\n",
    "}\n",
    "\n",
    "print(\"\\nAvailable Models:\")\n",
    "for i, name in enumerate(all_models, 1):\n",
    "    print(f\"{i}. {name}\")\n",
    "\n",
    "choice = int(input(\"\\nSelect a model by number: \"))\n",
    "model_key = list(all_models.values())[choice - 1]\n",
    "\n",
    "num_texts = int(input(\"How many texts would you like to analyze? \"))\n",
    "texts = [input(f\"Enter text {i+1}: \") for i in range(num_texts)]\n",
    "\n",
    "sentiments = predict_sentiment(texts, model_key)\n",
    "\n",
    "print(\"\\n=== Predictions ===\")\n",
    "for i, (text, sentiment) in enumerate(zip(texts, sentiments), 1):\n",
    "    print(f\"{i}. {text} â†’ {sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66e85aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('D:/saved_models/transformers/roberta-base\\\\tokenizer_config.json',\n",
       " 'D:/saved_models/transformers/roberta-base\\\\special_tokens_map.json',\n",
       " 'D:/saved_models/transformers/roberta-base\\\\vocab.json',\n",
       " 'D:/saved_models/transformers/roberta-base\\\\merges.txt',\n",
       " 'D:/saved_models/transformers/roberta-base\\\\added_tokens.json',\n",
       " 'D:/saved_models/transformers/roberta-base\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model.save_pretrained(f\"D:/saved_models/transformers/{model_name}\")\n",
    "tokenizer.save_pretrained(f\"D:/saved_models/transformers/{model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
